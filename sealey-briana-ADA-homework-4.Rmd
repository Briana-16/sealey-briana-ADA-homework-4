---
title: "sealey-briana-ADA-homework-4"
author: "Briana Sealey"
date: "5/10/2020"
output: html_document
---
```{r, warning = FALSE, message = FALSE}
#load packages
library(tidyverse)
library(patchwork)
library(devtools)
library(scales)
library(collapse)
library(BBmisc)
library(tidycovid19)
library(ggthemes)
library(AICcmodavg)
library(lme4)
theme_set(theme_tufte())
```

```{r, message=FALSE}
#Setup for challenges
merged <- download_merged_data(cached = TRUE)
merged <- merged %>%
  group_by(country, iso3c) %>%
  arrange(country, iso3c, date) %>%
  ## new code to replace NAs with zeros
  mutate(
    confirmed = ifelse(is.na(confirmed), 0, confirmed),
    deaths = ifelse(is.na(deaths), 0, deaths),
    recovered = ifelse(is.na(recovered), 0, recovered)
  ) %>%
  ## end of new code
  mutate(
    daily_confirmed = confirmed - lag(confirmed, n = 1),
    daily_deaths = deaths - lag(deaths, n = 1),
    daily_recovered = recovered - lag(recovered, n = 1)
  ) %>%
  mutate(
    daily_confirmed = replace_na(daily_confirmed, 0),
    daily_deaths = replace_na(daily_deaths, 0),
    daily_recovered = replace_na(daily_recovered, 0)
  ) %>%
  ungroup() %>%
  arrange(country, iso3c, date)

add_world1 <- merged %>%
  group_by(date) %>%
  arrange(date) %>%
  summarize(
    country = "World", iso3c = NA,
    confirmed = sum(confirmed, na.rm = TRUE),
    deaths = sum(deaths, na.rm = TRUE),
    recovered = sum(recovered, na.rm = TRUE),
    timestamp = fmode(timestamp)
  ) %>%
  mutate(
    daily_confirmed = confirmed - lag(confirmed, n = 1),
    daily_deaths = deaths - lag(deaths, n = 1),
    daily_recovered = recovered - lag(recovered, n = 1)
  ) %>%
  mutate(
    daily_confirmed = replace_na(daily_confirmed, 0),
    daily_deaths = replace_na(daily_deaths, 0),
    daily_recovered = replace_na(daily_recovered, 0)
  ) %>%
  ungroup() %>%
  arrange(country, iso3c, date)

add_world2 <- merged %>%
  group_by(country, iso3c) %>%
  summarize(
    population = fmode(population),
    land_area_skm = fmode(land_area_skm),
    timestamp = fmode(timestamp)
  ) %>%
  ungroup() %>%
  summarize(
    country = "World", iso3c = NA,
    population = sum(population, na.rm = TRUE),
    land_area_skm = sum(land_area_skm, na.rm = TRUE)
  ) %>%
  mutate(pop_density = population / land_area_skm)

add_world <- left_join(add_world1, add_world2, by = c("country", "iso3c"))
merged <- bind_rows(merged, add_world)

cv_data <- pivot_longer(merged,
  cols = c(
    "confirmed", "deaths", "recovered",
    "daily_confirmed", "daily_deaths", "daily_recovered"
  ),
  names_to = "variable", values_to = "cases"
) %>%
  arrange(country, variable, date) %>%
  rename(area = land_area_skm, density = pop_density) %>%
  mutate(rate = cases / population * 10^6) %>%
  ## new code to omit data before 2020-05-11
  filter(date < "2020-05-11")
## end of new code

#Running function
cv_summary <- function(d, country_list = "World",
                       plot = TRUE, facet = "country",
                       status = c("confirmed", "deaths", "recovered")) {

  # based on `wes_palettes()` GrandBudapest1, IsleofDogs1, IsleofDogs2
  # from the {wesanderson} package
  my_palette <- c(
    "#5B1A18", "#FD6467", "#F1BB7B", "#D67236",
    "#0F0D0E", "#9986A5", "#79402E", "#CCBA72", "#D9D0D3", "#8D8680",
    "#EAD3BF", "#AA9486", "#B6854D", "#39312F", "#1C1718"
  )

  if (facet == "country") {
    fill <- "variable"
    n <- length(unique(d$variable)) / 2
    # need only half of unique # of variable (3)
  }

  if (facet == "variable") {
    fill <- "country"
    n <- length(country_list)
    # need number of countries
  }

  if ("All" %in% country_list) {
    country_list <- unique(d$country)
    country_list <- setdiff(country_list, "World")
  }

  if ("World" %in% country_list) {
    d <- d %>% filter(country %in% country_list)

    totals <- d %>%
      group_by(variable) %>%
      summarize(
        country = "World",
        cases = max(cases),
        population = max(population),
        area = max(area),
        density = max(density),
        rate = max(rate, na.rm = TRUE),
        on = max(date)
      ) %>%
      select(country, variable, cases, population, area, density, rate, on) %>%
      arrange(variable) %>%
      ungroup()
  }

  if ("World" %nin% country_list) {
    d <- d %>% filter(country %in% country_list)
    totals <- d %>%
      group_by(country, variable) %>%
      summarize(
        cases = max(cases),
        population = max(population),
        area = max(area),
        density = max(density),
        rate = max(rate, na.rm = TRUE),
        on = max(date),
        gdp_capita = fmode(gdp_capita),
        income = fmode(income),
        life_expectancy = fmode(life_expectancy),
        max_sd = max(soc_dist),
        max_mr = max(mov_rest)
      ) %>%
      select(
        country, variable, cases, population, area, density, rate,
        gdp_capita, income, life_expectancy, max_sd, max_mr, on
      ) %>%
      arrange(country, variable) %>%
      ungroup()
  }

  if (plot == TRUE) {
    cc <- filter(d, variable %in% status)
    cum_cases_plot <- ggplot(
      data = cc,
      # use the tidy evaluation pronoun .data to slice the chosen fill
      # variable from the data frame
      aes(
        x = date, y = cases + 1, color = .data[[fill]],
        fill = .data[[fill]]
      )
    ) +
      geom_point(size = 0.5) +
      geom_line() +
      # use the tidy evaluation pronoun .data to slice the chosen facet_wrap
      # variable from the data frame
      facet_wrap(~ .data[[facet]], ncol = 5) +
      xlab("Date") +
      ylab("Log Cumulative Cases") +
      scale_y_log10(
        breaks = trans_breaks("log10", function(x) 10^x),
        labels = trans_format("log10", math_format(10^.x))
      ) +
      scale_color_manual(
        aesthetics = c("color", "fill"),
        name = NULL, values = my_palette
      )

    dc <- filter(d, variable %in% paste0("daily_", status))
    daily_cases_plot <- ggplot(
      data = dc,
      aes(
        x = date, y = cases, color = .data[[fill]],
        fill = .data[[fill]]
      )
    ) +
      geom_point(size = 0.5) +
      geom_line() +
      facet_wrap(~ .data[[facet]], ncol = 5) +
      xlab("Date") +
      ylab("Daily Cases") +
      scale_color_manual(
        aesthetics = c("color", "fill"),
        name = NULL, values = my_palette
      )
  }

  if (plot == TRUE) {
    return(list(
      totals = totals,
      cum_cases_plot = cum_cases_plot,
      daily_cases_plot = daily_cases_plot
    ))
  } else {
    return(list(totals = totals))
  }
}
```

##CHALLENGE 1
```{r}
cv_summary(cv_data)
```

###CHALLENGE 2
```{r}
#Figured out what the countries are named in the column using table
#table(cv_data$country)

#creating vector that includes on specified countries
G7 <- c("United States", "United Kingdom", "Canada", "France", "Germany", "China", "Russia", "Iran")

#Facet by country
cv_summary(cv_data, country_list = G7, facet = "country") 

#Facet by variable
cv_summary(cv_data, country_list = G7, facet = "variable") 

```

###CHALLENGE 3
```{r}
#datafrrame with all countries
all_summary <- cv_summary(cv_data, country_list = "All", plot = FALSE)
#turn the list into a dataframe
all_summary <- as.data.frame(all_summary$totals)

#Filter to create tibble d
all_summary %>%
  filter(population > 1000000) -> d
as_tibble(d)
head(d, 5)

```

###CHALLENGE 4
```{r}
d %>%
  filter(variable == "confirmed") -> overall
head(overall, 5)
top_overall <- overall[order(-overall$rate), ]
#Top 10
head(top_overall, 10)

d %>%
  filter(variable == "daily_confirmed") -> daily
head(daily, 5)
top_daily <- daily[order(-daily$rate), ]
#Top 10
head(top_daily, 10)

```

**Top 10 Based on overall rate:** Qatar, Spain, Ireland, Belgium, Singapore, US, Italy, Switzerland, UK, Bahrain

**Top 10 Based on daily rate:** Ecuador, Qatar, France, Ireland, Kuwait, Singapore, Belgium, Bahrain, Spain, Belarus

###CHALLENGE 5
```{r}
m0 <- lm(data = overall, rate ~ 1)
#Set up plot window
par(mfrow=c(1,2))
plot(m0$residuals) #not normal
qqnorm(m0$residuals) #not normal
shapiro.test(m0$residuals) #not normal
summary(m0) 

m1 <- lm(data = overall, rate ~ density + population + gdp_capita + income)
plot(m1$residuals) #not normal
qqnorm(m1$residuals) #not normal
shapiro.test(m1$residuals) #not normal
summary(m1)
#Density & gdp_capita are significant predictor variables, however this model is not normal

#Distribution of rate is skewed, needs to be transformed
#Turn plot window back to 1 plot per page
par(mfrow=c(1,1))
hist(overall$rate)

overall2 <- overall
#Replace 0's with NA in the dataset
overall2[overall2 == 0] <- NA
#I checked to make sure NA's were removed
#overall2$gdp_capita
m2 <- lm(data = overall2, log(rate) ~ log(density) * log(population) * log(gdp_capita) * income)
par(mfrow=c(1,2))
plot(m2$residuals) #normal
qqnorm(m2$residuals) #normal
shapiro.test(m2$residuals) #normal
#summary(m2) #not shown in html, very long because it includes all interactions
#After looking at summary, predictor variables are different from 0, but none of their estimates were significant

m3 <- lm(data = overall2, log(rate) ~ log(density) + log(population) + log(gdp_capita) + income)
par(mfrow=c(1,2))
plot(m3$residuals)
qqnorm(m3$residuals)
shapiro.test(m3$residuals)
summary(m3)
#The histogram is at least a little better wrt to distribution
par(mfrow=c(1,1))
hist(log(overall2$rate))
```

Looks like `m3` best describes the data. Log-transformed density & log-transformed gdp_capita are significant predictor variables of log-transformed rate.

###CHALLENGE 6
```{r, warning = FALSE, message = FALSE}
library(MASS)
step_full <- stepAIC(m3, scope = . ~ ., direction = "both")
detach(package:MASS)
```

Based on the stepwise AIC, a simpler model that includes only the **log-transformed density** and **log-transformed gdp_capita** are preferred. That is, these two parameters best explains the response variable. This is indicated by the Step AIC = 105.11, which is lower than the previous steps.

```{r, warning = FALSE, message = FALSE}
library(MuMIn)
m3 <- lm(data = overall2, log(rate) ~ log(density) + log(population) + log(gdp_capita) + income)
m4 <- lm(data = overall2, log(rate) ~ log(density) + log(gdp_capita))
#creating AIC table
(aic_table <- aictab(list(m3, m4), modnames = c("m3", "m4")))

r.squaredGLMM(m3)
r.squaredGLMM(m4)
```

Lools like `m4` best explains the greatest amount of variance in the dataset as calculated by AIC's. Furthermore, comparing the pseudo-$R^{2}$, `m3` is not significantly greater than `m4` (a 0.0061531 difference), thus the simpler model `m4` is still a better model for explaining the variance in the response variable.

###CHALLENGE 7
```{r}
#Adding in max_sd & max_mr
m5 <- lm(data = overall2, log(rate) ~ log(density) + log(gdp_capita) + max_sd)
m6 <- lm(data = overall2, log(rate) ~ log(density) + log(gdp_capita) + max_mr)
m7 <- lm(data = overall2, log(rate) ~ log(density) + log(gdp_capita) + max_sd + max_mr)

(aic_table <- aictab(list(m4, m5, m6, m7), modnames = c("m4", "m5", "m6", "m7")))
r.squaredGLMM(m4)
r.squaredGLMM(m5)
r.squaredGLMM(m6)
r.squaredGLMM(m7)
```

AIC calculation now deems `m7` as the best model and `m4` as the worst. However, the pseudo-$R^{2}$ values of `m5`, `m6` & `m4` are not significantly different than `m7`'s, thus the simpler `m4` is still sufficient enough in explaining the variance in log-transformed rate. This goes to show that relying purely on AIC values--or rather, one model selection test alone--is not enough to make a decision on which model is best; instead, more than one test should be done and compared before making a final decision.

###CHALLENGE 8
```{r, warning = FALSE}

#Filtering dataframe
d2 <- cv_data %>%
  filter(population > 1000000 & variable == "daily_confirmed" & rate > 0)

#I checked to make sure filter() worked
#d2$population
#d2$variable
#d2$rate

#Creating all models to be tested
#Null model with only random effects country + date
m0 <- lmer(data = d2, log(rate) ~ (1 | country) + (1 | date), REML = FALSE)

#Full model with density + gdp + soc + mov & random effects country + date
m1 <- lmer(data = d2, log(rate) ~ log(density) + log(gdp_capita) + soc_dist + mov_rest + (1 | country) + (1 | date), REML = FALSE)

#Nested model with density + gdp + soc & random effects country + date
m2 <- lmer(data = d2, log(rate) ~ log(density) + log(gdp_capita) + soc_dist + (1 | country) + (1 | date), REML = FALSE)
#Nested model with density + gdp + mov & random effects country + date
m3 <- lmer(data = d2, log(rate) ~ log(density) + log(gdp_capita) + mov_rest + (1 | country) + (1 | date), REML = FALSE)
#Nested model with density + soc + mov & random effects country + date
m4 <- lmer(data = d2, log(rate) ~ log(density) + soc_dist + mov_rest + (1 | country) + (1 | date), REML = FALSE)
#Nested model with density + gdp & random effects country + date
m5 <- lmer(data = d2, log(rate) ~ log(density) + log(gdp_capita) + (1 | country) + (1 | date), REML = FALSE)
#Nested model with density & random effects country + date
m6 <- lmer(data = d2, log(rate) ~ log(density) + (1 | country) + (1 | date), REML = FALSE)
#Nested model with gdp + soc + mov & random effects country + date
m7 <- lmer(data = d2, log(rate) ~ log(gdp_capita) + soc_dist + mov_rest + (1 | country) + (1 | date), REML = FALSE)
#Nested model with gdp + soc & random effects country + date
m8 <- lmer(data = d2, log(rate) ~ log(gdp_capita) + soc_dist + (1 | country) + (1 | date), REML = FALSE)
#Nested model with gdp + mov & random effects country + date
m9 <- lmer(data = d2, log(rate) ~ log(gdp_capita) + mov_rest + (1 | country) + (1 | date), REML = FALSE)
#Nested model with gdp & random effects country + date
m10 <- lmer(data = d2, log(rate) ~ log(gdp_capita) + (1 | country) + (1 | date), REML = FALSE)
#Nested model with soc + mov & random effects country + date
m11 <- lmer(data = d2, log(rate) ~ soc_dist + mov_rest + (1 | country) + (1 | date), REML = FALSE)

#AIC test
(aic_table <- aictab(list(m0, m1, m2, m3, m4, m5, m6, m7, m8, m9, m10, m11), modnames = c("m0","m1","m2","m3", "m4","m5", "m6", "m7", "m8", "m9", "m10", "m11")))

#Looks like m1 is best
summary(m1)

#Taking a peak at the runner-ups pseudo-R^2 values
r.squaredGLMM(m2)
r.squaredGLMM(m7)

#Pseudo R^2 associated with the best model
r.squaredGLMM(m1)
```

I also took a look at the pseudo-$R^{2}$ values for the runner-ups in the AIC test and based on this test alone, it looks like `m7` is a sufficient enough model to explain the variance in **log(rate)** because `m1`'s pseudo-$R^{2}$ is not significantly different from `m7` (difference of 0.0007523).

However, because the AIC test rank `m1` the best model, `m7` the 3rd best model, *and* the pseudo-$R^{2}$ is still higher for `m1` than `m7`, I'd conclude that all predictor variables provided in `m1` are necessary to fully capture the variance in the natural log of rate.